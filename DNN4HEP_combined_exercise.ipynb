{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrIxC4fJbf1V"
      },
      "source": [
        "# Neural Network Classifier for Particle Physics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB9pKp11bf1W"
      },
      "source": [
        "In this exercise, we'll build a PyTorch binary classifier DNN, and apply this DNN to separating a Higgs Boson signal from a background process.\n",
        "\n",
        "The signal process is a Higgs Boson decaying to two leptons among other stuff. The background process is a pair of top quarks decaying to a very similar final state, two leptons plus other particles, that look on a first glance essentially the same in the detector.\n",
        "\n",
        "Each dataset contains 26 features related to the particles detected in each case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMXrFMmEbf1W"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luVf64Ocbf1X"
      },
      "source": [
        "## Importing Data\n",
        "\n",
        "The first thing to do is to load your data. It is provided for you on CERNBox. You should be able to access it. If problems arise, contact one of the tutors.\n",
        "\n",
        "So, to download the input data execute the next code block."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://cernbox.cern.ch/remote.php/dav/public-files/icjK5HWChdTcdb2/WW_vs_TT_dataset.h5\"\n",
        "\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# download\n",
        "response = requests.get(url)\n",
        "response.raise_for_status() # Check for download errors\n",
        "\n",
        "# load into a in-memory binary stream\n",
        "H_vs_TT_dataset = io.BytesIO(response.content)\n"
      ],
      "metadata": {
        "id": "YgaJ9MdohvlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is stored in `h5` format which is the industry standard for storing numpy arrays in machine learning contexts.\n",
        "\n",
        "The file contains two datasets, `Signal` and `Background`. Each dataset is an array of shape N x 26, where N is the number of separate proton-proton collisions (we call these \"events\") and 26 refers to the number of features stored in that file.\n",
        "\n",
        "We can load both datasets as [pandas](https://pandas.pydata.org/docs/user_guide/index.html) dataframes."
      ],
      "metadata": {
        "id": "XSd67Rk4h5m_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6OGG2hebf1X"
      },
      "outputs": [],
      "source": [
        "# Load file\n",
        "file = h5py.File(H_vs_TT_dataset, 'r')\n",
        "\n",
        "# Extract data\n",
        "df_signal       = pd.DataFrame(file['Signal'][:])\n",
        "df_background   = pd.DataFrame(file['Background'][:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaeGavONbf1X"
      },
      "source": [
        "## Data Visualisation\n",
        "\n",
        "As we stated in the lectures, it's useful to understand the input parameters and compare them between the different classes to get a feeling for the actual task.\n",
        "\n",
        "Remember you are supposed to design a classifier that\n",
        "\n",
        "The following function can be used to draw histograms of a certain feature for the two classes in order to compare them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orNQD0e-bf1X"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def compare_distributions(signal_data, background_data, variable_name):\n",
        "    plt.figure(figsize=(10, 6),dpi=100)\n",
        "    plt.hist(signal_data[variable_name], bins=40,  histtype='step', label='Signal', density=True)\n",
        "    plt.hist(background_data[variable_name], bins=40, histtype='step', label='Background', density=True)\n",
        "    plt.xlabel(variable_name)\n",
        "    plt.ylabel('Density')\n",
        "    plt.title(f'Distribution of {variable_name}')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as an exaple let's have a look at one of the leptons' energy."
      ],
      "metadata": {
        "id": "wpYOLCC3m442"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7qprWf7bf1Y"
      },
      "outputs": [],
      "source": [
        "compare_distributions(df_signal, df_background, 'lepton0_energy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question!\n",
        "\n",
        "Do you think with this information alone you could reliably distinguish between signal and background?"
      ],
      "metadata": {
        "id": "khbfYA0HmlnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, there are some differences in the feature distributions comparing the two classes. But they aren't huge and overlap quite a bit. So a classifier using this would probably not suffice for anything practical. You have to include addiditonal information.\n",
        "\n",
        "Here is a list of all the other features stored in the dataset:\n",
        "```python\n",
        "['lepton0_px', 'lepton0_py', 'lepton0_pz', 'lepton0_energy',\n",
        "'lepton1_px', 'lepton1_py', 'lepton1_pz', 'lepton1_energy',\n",
        "'jet0_px', 'jet0_py', 'jet0_pz', 'jet0_energy',\n",
        "'jet1_px', 'jet1_py', 'jet1_pz', 'jet1_energy',\n",
        "'Njets', 'HT_all', 'MissingEnergy',\n",
        "'lepton0_mass', 'lepton1_mass', 'jet0_mass', 'jet1_mass', 'combined_leptons_mass',\n",
        "'angle_between_jets', 'angle_between_leptons']\n",
        "```\n",
        "Try to plot some of them and pick the ones you think are suitable for separating the signal from background."
      ],
      "metadata": {
        "id": "346gikKSqAjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add the features you want to train on\n",
        "list_of_selected_features = []\n",
        "for feature in list_of_selected_features:\n",
        "    compare_distributions(df_signal, df_background, feature)"
      ],
      "metadata": {
        "id": "Z84n69pGqqxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pViTeOqvbf1Y"
      },
      "source": [
        "## Data Preparation Task\n",
        "\n",
        "Now, since you have a feeling for the input data now, it is time to become active:\n",
        "\n",
        "1. Create two arrays based on the `input_features` you wish to train on. We have to use the same features for signal and background.\n",
        "2. Create the targets for the learning: Label 0 for background, and label 1 for signal. Hint:\n",
        "3. Concatenate the signal and background input data filtered for the features you would like to use into one big `input_data` array. Do the same for the targets. Shuffle the numpy arrays. Careful: Do not shuffle before you have connected the targets to the `input_data`.\n",
        "4. Split the input and target data into training, validation and testing blocks with a ratio of 80% : 10% : 10%. Hint: you can use [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "5. Normalise the data. It's good practice to just use a scaling derived on the training dataset.\n",
        "6. Convert all arrays to torch.tensors.\n",
        "7. Check that the tensors look sensible: correct shape, shuffled etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the features you are interested in\n",
        "input_features = [] # fill the features you want to train on\n",
        "df_signal_filtered = df_signal[input_features]\n",
        "df_background_filtered = df_background[input_features]"
      ],
      "metadata": {
        "id": "2-0OY1aMtKBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg2-yc7Gbf1Z"
      },
      "outputs": [],
      "source": [
        "# Set targets for training\n",
        "y_signal     = np.ones(len(df_signal_filtered))\n",
        "y_background = np.zeros(len(df_background_filtered))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhyy9ArSbf1Z"
      },
      "outputs": [],
      "source": [
        "# Combine the dataframes as one big numpy array\n",
        "input_data = np.concatenate((df_signal_filtered, df_background_filtered), axis=0)\n",
        "target     = np.concatenate((y_signal, y_background), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_olh-2ULbf1Z"
      },
      "outputs": [],
      "source": [
        "# split data into train, validation, and test sets (You can also do the shuffle here, if not shuffled before)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ... your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### $\\color{red}{\\text{This is for the Mastering Model Building exercise. Skip this block in your first pass!}}$"
      ],
      "metadata": {
        "id": "86_Fjl2LxaHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have learned that normalizing the data can lead to benefits during training. Try it out using the [`StandardScaler` from `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
      ],
      "metadata": {
        "id": "TJ-blNhyypIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWR0mHB7bf1a"
      },
      "outputs": [],
      "source": [
        "# Scale the data to lie between -1 and 1 using sklearn StandardScaler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n",
        "X_test  = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### These are now your tensors for training"
      ],
      "metadata": {
        "id": "FxHsUDiBxnrK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPthHDA1bf1a"
      },
      "outputs": [],
      "source": [
        "# Convert all the data sets into the PyTorch tensor format, e.g.\n",
        "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "# ...\n",
        "\n",
        "# ... your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSm-5B-Wbf1a"
      },
      "source": [
        "## Training Task\n",
        "\n",
        "1. Define a [`nn.Sequential` model](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). The input linear layer should be of the size `Nfeatures = len(input_features)`. Each linear layer should be succeeded by an activation function. The final activation function should be appropriate for binary classification. Hint: Something with limited value space!\n",
        "\n",
        "2. Define a loss function and optimiser. Which loss function should we use for binary classification? There are a couple of [loss functions available in PyTorch](https://pytorch.org/docs/stable/nn.html#loss-functions). You can use [Stochastic Gradient Descent (SGD)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) for now as an optimizer with a learning rate `lr=0.25`.\n",
        "\n",
        "3. Write a training loop, inspired by the previous exercise. Make sure to track the training loss as well as the validation loss.\n",
        "\n",
        "4. Plot the loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NiCIA5sbf1a"
      },
      "outputs": [],
      "source": [
        "# Just write this as nn.Sequential\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(\n",
        "    # ... add your model definition here\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZlGzQPGbf1a"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "import torch.optim as optim\n",
        "criterion = # ... your code goes here\n",
        "optimizer = # ... your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### $\\color{red}{\\text{This is for the Mastering Model Building exercise. Skip this block in your first pass!}}$"
      ],
      "metadata": {
        "id": "QtNcutDv23JF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might have noticed that the training is not as smooth as you naively expected. Try playing around with the parameters of the SGD optimizer, e.g. the learning rate. Does this change something?"
      ],
      "metadata": {
        "id": "sX36NA-D3FGb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4PbSGuo5uT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might also think about adding some momentum to the default SGD. Note that the [PyTorch implementation of SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) already supports two kinds of momenta."
      ],
      "metadata": {
        "id": "QZDEkxhK40GU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ITSdbN6Y5xfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another option might be to use some adaptive learning. Use the [Adagrad implementation in PyTorch](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad)."
      ],
      "metadata": {
        "id": "TXlZhyCz5q6w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A-wdeP9_6KvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, try combining adaptive learning with momentum. The [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) does just that! It is the default optimizer for most ML tasks and tends to often turn out as the most optimal choice! Just use Adam from now on as a first choice."
      ],
      "metadata": {
        "id": "rnLK7zv-6N-r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_DrdYD44jX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to continue playing around with the optimizers.\n"
      ],
      "metadata": {
        "id": "QmFWoNhp7bTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### $\\color{red}{\\text{This is for the Mastering Model Building exercise. Skip this block in your first pass!}}$"
      ],
      "metadata": {
        "id": "cbVp6D92dsuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking through the documentation of the optimizers, you might have noticed, that they provide you with the option to add a regularization term to your loss.\n",
        "\n",
        "Try it!"
      ],
      "metadata": {
        "id": "6G3GlvRrdxHR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvXQbeIverLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_5UnclIbf1a"
      },
      "source": [
        "#### Finally, the training loop\n",
        "Following on from the syntax we had before, we can pass all our data as a torch tensor to our model. Then iterate over the training data a number of epochs.\n",
        "\n",
        "We also include a validation step at the end of each training epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCRLRNHsbf1a"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses   = []\n",
        "\n",
        "N_epochs = 50\n",
        "\n",
        "for epoch in range(N_epochs):\n",
        "\n",
        "    model.train()\n",
        "    # tell the optimizer to begin an optimization step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    # use the model as a prediction function for the training data:\n",
        "    # features → prediction\n",
        "\n",
        "    predictions = # ... your code goes here\n",
        "\n",
        "    # compute the loss (χ²) between these predictions and the intended targets\n",
        "\n",
        "    loss = # ... your code goes here\n",
        "\n",
        "    # tell the loss function and optimizer to end an optimization step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # save the training loss value of this epoch\n",
        "    train_losses.append(loss.item())\n",
        "    print(f'Epoch [{epoch + 1}/{N_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # add in the validation loss part\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        # get the prediction of your model on the validation data\n",
        "\n",
        "        val_predictions = # ... your code goes here\n",
        "\n",
        "        # compute the loss on the validation data\n",
        "\n",
        "        val_loss = #. ... your code goes here\n",
        "\n",
        "        val_losses.append(val_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### $\\color{red}{\\text{This is for the Mastering Model Building exercise. Skip this block in your first pass!}}$"
      ],
      "metadata": {
        "id": "67JE7Qbt-5hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might have noticed that in each epoch you are running over the whole training dataset at once.\n",
        "While this is computationally very fast, for large datasets that might become a problem, since you are loading the whole dataset into memory.\n",
        "While this is not an issue in this exercise, you might keep this in mind.\n",
        "More important here, an optimization in smaller increments might improve the convergence (although it is strongly tied to what learning rate you use).\n",
        "\n",
        "Therefore, try training in mini-batches.\n",
        "Here again, PyTorch makes that easy for us.\n",
        "Utilize the [`TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) and [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) classes to create your data that can be loaded in batches.\n",
        "\n",
        "Adjust the `batch_size` as you wish. What do you observe?"
      ],
      "metadata": {
        "id": "uSBRTtgh_Ljz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2ZulrIxbf1b"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets that combine input features with labels\n",
        "\n",
        "# ... your code goes here\n",
        "\n",
        "# Create DataLoader objects for training, validation, and testing in batches\n",
        "\n",
        "# ... your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind, that the parameters of your model have already been pretrained in previous runs and the status is stored. If you want to reset them, meaning reinitializing the model, either rerun the cell where the model is defined, or reset the parameters using this custom function:"
      ],
      "metadata": {
        "id": "THATZNTqDatV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_weights(m):\n",
        "    if hasattr(m, 'reset_parameters'):\n",
        "        m.reset_parameters()\n",
        "\n",
        "model.apply(reset_weights)"
      ],
      "metadata": {
        "id": "uX_IuGy2DaJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, adjust your training loop to run over the batches of data."
      ],
      "metadata": {
        "id": "YZJaAf5QBUnD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy4cqIDsbf1b"
      },
      "outputs": [],
      "source": [
        "# Training loop on mini-batches with loss stored for plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    # run over batches of data\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = # ... your code goes here\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # compute average loss over all batches\n",
        "    train_loss = # ... your code goes here\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # No compute the batchwise validation loss\n",
        "    model.eval()\n",
        "    val_loss = # ... your code goes here\n",
        "    # ... your code goes here\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now let's have a look at the loss function\n",
        "\n",
        "Plot the loss values you obtained for both the training and validation data over the training epochs.\n",
        "\n",
        "What do you observe? Did the training converge? Do we need to run more epochs?"
      ],
      "metadata": {
        "id": "fbvhRarh_Etj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss function for training and validation sets\n",
        "import matplotlib.pyplot as plt\n",
        "# ... define what to plot\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train Loss', 'Validation Loss'])"
      ],
      "metadata": {
        "id": "eSeO19q-_Rsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### $\\color{red}{\\text{This is for the Mastering Model Building exercise. Skip this block in your first pass!}}$"
      ],
      "metadata": {
        "id": "Rug2ODq5F1QY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might observe that your training hasn't converged yet, and you need to increase the number of training epochs.\n",
        "\n",
        "It might also be that your model did converge, but you observe serious overtraining by comparing the training with the validation loss.\n",
        "\n",
        "Or your convergence is just fine, and you don't see overtraining, but your model performance sucks.\n",
        "\n",
        "In any case, try to improve your model training by what you have learned in the lecture. The red sections should provide you with some guidance, but feel free to add whatever you consider appropriate. Have fun!"
      ],
      "metadata": {
        "id": "SewPStQ2F-oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "So, how does our model perform? The losses only tell us that the model learns something. But it is unclear how well it does.\n",
        "\n",
        "We need to evaluate this on an idependent dataset. Luckily, you have prepared an independent test dataset above!"
      ],
      "metadata": {
        "id": "JPTgC8aOBg6S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXONBfTqbf1c"
      },
      "source": [
        "1. Evaluate the model on the testing dataset.\n",
        "3. Plot the distribution of scores, for each, true Signal and Background classes.\n",
        "4. Compute various metrics\n",
        "5. Plot ROC curve (example code given)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY5kdQa7bf1c"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test dataset\n",
        "# per default  PyTorch will add the predicted values to the computation graph\n",
        "# Call the detach() method to remove them\n",
        "\n",
        "# ... your code goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz_6VnRVbf1c"
      },
      "outputs": [],
      "source": [
        "# Filter the predicted events for true Signal and true Background\n",
        "\n",
        "# ... your code goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXIJd7uYbf1c"
      },
      "outputs": [],
      "source": [
        "# Plot the predicted scores for both true Signal and true Background events\n",
        "import matplotlib.pyplot as plt\n",
        "# ... define what to plot\n",
        "plt.legend([\"true Signal\",\"true Background\"])\n",
        "plt.xlabel(\"Predicted Scores\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.xlim((0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question!\n",
        "\n",
        "From this visual help: Was your model able to distinguish Signal from Background?"
      ],
      "metadata": {
        "id": "Ax-ELY0ON7RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantify how well your model did\n",
        "\n",
        "Let's now compute the accuracy and some other metrics that quantify how well your model distinguished between Signal and Background.\n",
        "\n",
        "Use the implementations for the metrics from [`sklearn.metrics`](https://scikit-learn.org/stable/api/sklearn.metrics.html)."
      ],
      "metadata": {
        "id": "WkQy4ipGOeIR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsCJeilzbf1d"
      },
      "outputs": [],
      "source": [
        "# sklearn.metrics require exact value matches, so we need to round to 0 or 1\n",
        "\n",
        "final_prediction = np.round(\n",
        "    #... your code goes here\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLUvOVgkbf1d"
      },
      "outputs": [],
      "source": [
        "# Compute the accuracy_score\n",
        "\n",
        "accuracy = # ... your code goes here\n",
        "print(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuPzD8Anbf1d"
      },
      "outputs": [],
      "source": [
        "# Caclulate some other metrics that are suitable for a classification\n",
        "\n",
        "# ... your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Receiver Operating Characteristic (ROC) curve\n",
        "\n",
        "A common method in order to quantify how well a classifier performs is to look on the rate of false positives and true positive classifications.\n",
        "The so-called Receiver Operating Characteristic (ROC) combines these into a single visual representation by plotting the two values in a 2D scatter.\n",
        "\n",
        "However, with our NN model we have trained above, we can create an almost infinite number of classifiers (restricted only by the numerical precision), by varying the `Predicted Score` value at which we define the border between Signal and Background.\n",
        "\n",
        "This gives us a quasi-continuous set of points in the aforementioned 2D scatter, a curve. We call this the ROC curve.\n",
        "This is a common visual tool for comparing different classifier models, but we can also derive a quantifyable number related to this by computing the integral under this curve. Since it is measuring the **a**rea **u**nder the **c**urve, we call it ROC-AUC.\n",
        "\n",
        "Again, we can use the implementations from sklearn.metrics for the `roc_curve` and also to compute the `auc`.\n",
        "\n",
        "Plot the ROC curve and compute the ROC-AUC!"
      ],
      "metadata": {
        "id": "qweX0eRVRI0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxA5JsO0bf1d"
      },
      "outputs": [],
      "source": [
        "# Compute ROC curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Compute the ROC\n",
        "\n",
        "# ... your code goes here\n",
        "\n",
        "# Compute AUC (Area Under Curve)\n",
        "\n",
        "# ... your code goes here\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "# ... define what to plot\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question!\n",
        "\n",
        "How does your classifier perform in comparison to a random classification that assigns randomly 0 or 1 to an event with equal probability?"
      ],
      "metadata": {
        "id": "cqqSBLbPUQ6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### $\\color{red}{\\text{This is for the Mastering Model Building exercise. Skip this block in your first pass!}}$"
      ],
      "metadata": {
        "id": "H0gWDlDSU-YJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might have seen now, that even with a converging NN model your classification performance is far from optimal. Even if it is already very good, can you do better?\n",
        "\n",
        "Try to improve your model by adjusting the architecture.\n",
        "- Try using more layers or more nodes per layer.\n",
        "- Review your choices for the activation functions.\n",
        "- Also try to review what input features you feed the model.\n",
        "\n",
        "Try whatever comes to your mind and you consider appropriate.\n",
        "\n",
        "If you feel stuck, or you are bored, you might also find the following bonus tasks worth your time.\n",
        "They might help you with winkling out the last bits of performance improvement."
      ],
      "metadata": {
        "id": "NY3IGbTfVATx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\color{red}{\\text{Bonus Tasks}}$\n"
      ],
      "metadata": {
        "id": "70uSJgwXJO2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Implement Early Stopping for your model training"
      ],
      "metadata": {
        "id": "jQIviO1AXaEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to not manually tune the number of training epochs to avoid overtraining and find the optimal stopping point, try to implement early stopping into your model training.\n",
        "\n",
        "To do so, use the provided class, but feel free to adjust whatever you like:"
      ],
      "metadata": {
        "id": "DIcOIdmNXirI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience        # How many epochs to wait\n",
        "        self.min_delta = min_delta      # Minimum improvement to count\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0  # Reset counter if improvement\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "HJ-WmXWrX_Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And add the early stopping into your training loop. You could do it like this for example:\n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # --- Train ---\n",
        "      .\n",
        "      .\n",
        "      .\n",
        "    # --- Validate ---\n",
        "      .\n",
        "      .\n",
        "      .\n",
        "    # --- Check Early Stopping ---\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "```\n",
        "Feel free to adjust it to your needs."
      ],
      "metadata": {
        "id": "rRm9zxqQYGfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Adjust the learning rate during training"
      ],
      "metadata": {
        "id": "SRI7mSIRbsNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It might happen that at some point your training doesn't improve anymore and your losses are oscillating around.\n",
        "This might be the perfect opportunity to check whetehr dynamically changing the learning rate might get you even closer to the minimum.\n",
        "\n",
        "Use the instructions from the [PyTorch tutorial \"How to adjust learning rate\"](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) to add a learning rate scheduler (or multiple) that dynamically changes the learning rate."
      ],
      "metadata": {
        "id": "m85pbOR1b7rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Use Dropout"
      ],
      "metadata": {
        "id": "wZeXx6fdUC-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you encounter overfitting, it might have many reasons.\n",
        "One of the possibilities is that your network kind of memorizes your training data, and/or certain parts of your network become more important than the rest of the network.\n",
        "\n",
        "One way to reduce this is to randomly turn off parts of the network before a training batch is passed through the network during training. This process is called `Dropout`.\n",
        "\n",
        "Implementing it is as simple as adding [PyTorch `Dropout(p)` layers](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html), with `p` being the dropout probability, to your PyTorch `Sequential` model:\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    ...\n",
        "    nn.Linear(N, M), # Layer of nodes\n",
        "    nn.Sigmoid(), # Activation function\n",
        "    nn.Dropout(p) # Dropout layer\n",
        "    nn.Linear(M, O), # Layer of nodes\n",
        "    ...\n",
        ")\n",
        "```\n",
        "\n",
        "Try to add it into your classification model and see what it changes during the training and in the performance you can achieve."
      ],
      "metadata": {
        "id": "XC4nNg-JikC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Study the feature importance of your model"
      ],
      "metadata": {
        "id": "rX5eq4idWkD3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h2omRnBSsRP"
      },
      "source": [
        "The following method is a simple way of calculating the feature importance of the input dataset. It shuffles the values of one feature at a time into a random order (imagine the `df_signal` dataframe but with all the values in one column randomly shuffled). Since the values of this feature are now not correlated with the rest of the features in that datapoint (event), the performance should change. The extend to which the performance changes is a measure of the feature importance: if you shuffle `feature1` and very little changes, this tells us that `feature1` was not important to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTu5p1i5bf1d"
      },
      "outputs": [],
      "source": [
        "# Compute feature importance of 26 input features to DNN\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def permutation_importance(model, X_val_tensor, y_val):\n",
        "\n",
        "    detach_to_binary = lambda x: np.round(model(x).detach().numpy())\n",
        "\n",
        "    baseline_score = accuracy_score(y_val, detach_to_binary(X_val_tensor))  # For a classification task\n",
        "    importances = []\n",
        "\n",
        "    for feature_idx in range(X_val_tensor.shape[1]):\n",
        "        # Shuffle the values of the current feature\n",
        "        X_val_shuffled = X_val_tensor.clone()\n",
        "        X_val_shuffled[:, feature_idx] = X_val_shuffled[:, feature_idx][torch.randperm(X_val.shape[0])]\n",
        "\n",
        "        # Recalculate the performance\n",
        "        score_shuffled = accuracy_score(y_val, detach_to_binary(X_val_shuffled))\n",
        "        print(score_shuffled)\n",
        "\n",
        "        # The difference in performance is the importance of this feature\n",
        "        importances.append(baseline_score - score_shuffled)\n",
        "\n",
        "    return np.array(importances)\n",
        "\n",
        "# Usage example\n",
        "importances = permutation_importance(model, X_val_tensor, y_val_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nv8kZuUbf1d"
      },
      "outputs": [],
      "source": [
        "# sort features according to importance\n",
        "idx = np.argsort(np.asarray(importances))\n",
        "\n",
        "sorted_features_descending = df_background.columns.to_numpy()[idx]\n",
        "\n",
        "for i in range(len(sorted_features_descending)):\n",
        "    print(f\"Feature: {sorted_features_descending[i]}, Importance: {importances[idx[i]]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN_g-tdRbf1e"
      },
      "outputs": [],
      "source": [
        "# Plot feature importances\n",
        "import matplotlib.pyplot as plt\n",
        "importances = importances[idx]\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(range(len(importances)), importances, align='center')\n",
        "plt.yticks(range(len(importances)), sorted_features_descending)\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePNWtK4cbf1e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiD8x5QzSsRQ"
      },
      "source": [
        "#### 5. Use the `nn.Module` functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EV2qcmaSsRQ"
      },
      "source": [
        "So far we have defined our models using `nn.Sequential`, which is a concise and readable syntax for MLPs.\n",
        "\n",
        "For more complex models, or architectures which will contain multiple MLPs in different roles, it may be preferable to use PyTorch's `nn.Module` which defines a Python class. `nn.Module` is much more powerful and flexible, although contains more \"boiler-plate code\". It is instructive good to see this syntax and become familiar with \"overriding\" base PyTorch classes.\n",
        "\n",
        "The class contains:\n",
        "1. An `__init__` method (as required by all Python classes if you know OOP-Python). Here we define our `Linear` layers.\n",
        "2. A `forward` method that controls the forward flow of data through the network. Here we order our `Linear` layers and apply our activation functions. (The preferred approach is to include the activation functions in the `forward` method rather than the `__init__`)\n",
        "\n",
        "The code below defines a class `SimpleDNN` which has two layers. Make sure you understand what this code is doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgtxpRkajgeE"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleDNN(nn.Module):\n",
        "    def __init__(self, N_input_features): # You can add more parameters here, such that the size of all layers can be\n",
        "        # defined in the constructor\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(SimpleDNN, self).__init__()\n",
        "        self.linear1 = nn.Linear(N_input_features, 50)\n",
        "        self.linear2 = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        # Compute the forward pass.\n",
        "        # The first layer is self.linear1, then we apply the ReLU activation function\n",
        "        x1     = F.relu(self.linear1(x))\n",
        "        # The second layer is self.linear2, then we apply the sigmoid activation function to get our final output\n",
        "        y_pred = F.sigmoid(self.linear2(x1))\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNiPhH0eSsRQ"
      },
      "source": [
        "You then call your model in exactly the same way as we did when `model = nn.Sequential(...)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX25gRsbSsRQ"
      },
      "outputs": [],
      "source": [
        "model = SimpleDNN(3)\n",
        "model(X_train_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbKq4pNjSsRQ"
      },
      "source": [
        "Task: can you re-write your best performing DNN using `nn.Module`, and can include dropout in the `__init__`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}